# Museum Multi-modal QA Demo

## 项目简介
多模态图像理解与语音问答系统开发
构建适用于博物馆场景的多模态图文交互系统，实现展品图像理解、问答生成、知识检索与语音播报全流程闭环。
技术栈：Python, Qwen2.5-VL, CLIP, FAISS, Gradio, Edge-TTS
• 自主构建数据集： 编写爬虫脚本，自动抓取多个博物馆官网的展品图像及文字介绍，整理为结构化图文数据库，涵盖数百张展品图与描述；
• 图像语义检索：利用 CLIP 模型提取图像语义特征，结合 FAISS 构建图像向量索引库，支持用户上传图像后进行语义相似展品的快速检索；
• 多模态问答交互：基于 Qwen2.5-VL 模型实现展品图像自动生成中文描述和问答能力，能够回答自然语言问题；
• 引入本地文献知识库：通过向量化匹配补充展品相关背景资料，提升用户理解深度，实现图像、问答与知识的联动呈现；
• 使用 Gradio 搭建完整可交互前端，整合图像上传、自动描述生成、知识展示和问答模块；并通过 Edge-TTS 实现语音播报功能，优化用户体验。

## 技术栈
- Python, CLIP, FAISS, Qwen2.5-VL, Gradio, Edge-TTS

## 快速开始
1. 安装依赖：
   ```bash
   pip install -r requirements.txt
