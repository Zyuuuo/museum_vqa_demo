# Day 1: é¡¹ç›®åˆå§‹åŒ– + å›¾åƒæ•°æ®é€‰å‹ + CLIP å›¾åƒæ¦‚å¿µåŒ¹é… Demo

import torch
from PIL import Image
from transformers import CLIPProcessor, CLIPModel
import requests
import matplotlib.pyplot as plt

# 1. è½½å…¥é¢„è®­ç»ƒ CLIP æ¨¡å‹
model_path = '/root/autodl-tmp/Hugging-Face/hub/models--openai--clip-vit-base-patch32/snapshots/3d74acf9a28c67741b2f4f2ea7635f0aaf6f0268'
model = CLIPModel.from_pretrained(model_path)
processor = CLIPProcessor.from_pretrained(model_path)

# 2. ç¤ºä¾‹ï¼šåšç‰©é¦†å±•å“å›¾åƒï¼ˆå¯æ›¿æ¢ä¸ºä»»æ„å…¬å¼€æ–‡åŒ–é—äº§å›¾åƒï¼‰
image_path = '/root/autodl-tmp/Multimodal_heritage/data/images/1.jpg'
image = Image.open(image_path).convert("RGB")

candidate_texts = [
    "an ancient Egyptian statue",
    "a Pharaoh sculpture",
    "a bust of Akhenaten",
    "a Roman emperor bust",
    "a Mesopotamian stone carving",
    "a sandstone religious figure",
    "a medieval European knight statue"
]


# 4. æ¨¡å‹å¤„ç†è¾“å…¥
inputs = processor(text=candidate_texts, images=image, return_tensors="pt", padding=True)

# 5. æ¨ç†å¾—åˆ°å›¾åƒä¸æ–‡æœ¬çš„ç›¸ä¼¼åº¦åˆ†å¸ƒ
with torch.no_grad():
    outputs = model(**inputs)
    logits = outputs.logits_per_image.softmax(dim=1)[0]  # softmax æ¦‚ç‡

# 6. è¾“å‡ºæ’åºç»“æœ
print("\n[CLIP å›¾åƒè¯­ä¹‰åŒ¹é…ç»“æœ]")
ranked = sorted(zip(candidate_texts, logits), key=lambda x: -x[1])
for text, score in ranked:
    print(f"{text:<30}  ->  score: {score.item():.4f}")

# 7. å±•ç¤ºå›¾åƒ
plt.imshow(image)
plt.axis("off")
plt.title("Museum Artifact Image")
plt.show()

"""
âœ… ä»Šæ—¥ç›®æ ‡ï¼š
- æˆåŠŸè¿è¡Œ CLIP æ¨¡å‹å¯¹åšç‰©é¦†å›¾åƒè¿›è¡Œå¤šæ ‡ç­¾è¯­ä¹‰åˆ†ç±»ï¼ˆå›¾â†’ç±»å±ï¼‰
- ä¸ºåç»­æ¨¡å—ï¼ˆBLIP caption / VQAï¼‰æä¾›è¯­ä¹‰åˆç­›èƒ½åŠ›

ğŸ“Œ æ˜æ—¥ä»»åŠ¡é¢„å‘Šï¼ˆDay 2ï¼‰ï¼šä½¿ç”¨ BLIP-2 è‡ªåŠ¨ç”Ÿæˆå›¾åƒæè¿° + å›¾åƒé—®ç­”ç³»ç»Ÿæ¡†æ¶
"""
